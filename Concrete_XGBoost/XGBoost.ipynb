{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost Model for Predicting Concrete Compressive Strength\n",
    "\n",
    "In this notebook, we will create and evaluate an XGBoost model to predict the compressive strength of concrete based on various features. The workflow includes loading the data, preprocessing, feature engineering, hyperparameter tuning using GridSearchCV, and evaluating the model's performance.\n",
    "\n",
    "---\n",
    "*Created: Md. Rafiquzzaman Rafi*\n",
    "\n",
    "*Date: 27 August, 2024*\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Import Libraries\n",
    "\n",
    "We start by importing the necessary libraries for data manipulation, model training, and evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Load the Dataset\n",
    "\n",
    "Next, we load the dataset containing the concrete mix components and their corresponding compressive strengths.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv('Concrete_Data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Rename Columns for Easier Access\n",
    "\n",
    "We rename the columns to shorter names for easier access and manipulation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns for easier access\n",
    "data = data.rename(columns={\n",
    "    'Cement (component 1)(kg in a m^3 mixture)': 'cement',\n",
    "    'Blast Furnace Slag (component 2)(kg in a m^3 mixture)': 'blast_furnace_slag',\n",
    "    'Fly Ash (component 3)(kg in a m^3 mixture)': 'fly_ash',\n",
    "    'Water  (component 4)(kg in a m^3 mixture)': 'water',\n",
    "    'Superplasticizer (component 5)(kg in a m^3 mixture)': 'superplasticizer',\n",
    "    'Coarse Aggregate  (component 6)(kg in a m^3 mixture)': 'coarse_aggregate',\n",
    "    'Fine Aggregate (component 7)(kg in a m^3 mixture)': 'fine_aggregate',\n",
    "    'Age (day)': 'age',\n",
    "    'Concrete compressive strength(MPa, megapascals) ': 'compressive_strength'\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Create Additional Features\n",
    "\n",
    "We create two new features that might help in predicting compressive strength: the ratio of cement to coarse aggregate and the ratio of cement to fine aggregate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create additional features\n",
    "data['cement_coarse'] = data.cement / data.coarse_aggregate\n",
    "data['cement_fine'] = data.cement / data.fine_aggregate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Define Features and Target Variable\n",
    "\n",
    "We separate the features (X) from the target variable (y). The target variable in this case is the compressive strength of the concrete.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and target variable\n",
    "X = data.drop(['compressive_strength'], axis=1)\n",
    "y = data['compressive_strength']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Feature Scaling\n",
    "\n",
    "We apply standard scaling to the features to normalize them, which is particularly important for models like XGBoost.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Split Data into Training and Test Sets\n",
    "\n",
    "We split the data into training and test sets, using 80% of the data for training and 20% for testing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Create the XGBoost Regressor Model\n",
    "\n",
    "We initialize the XGBoost regressor with some basic parameters. The `objective` is set to `reg:squarederror` as it's a regression problem, and the `eval_metric` is set to `rmse`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the XGBoost regressor model\n",
    "model = xgb.XGBRegressor(objective='reg:squarederror', eval_metric='rmse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Define Hyperparameters for Tuning\n",
    "\n",
    "We define a grid of hyperparameters that we want to tune using GridSearchCV. This includes the number of estimators, the maximum depth of trees, the learning rate, and the subsample ratio.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters to tune\n",
    "param_grid = {\n",
    "    'n_estimators': [1000, 2000],\n",
    "    'max_depth': [3, 6, 9],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'subsample': [0.8, 1.0]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. Hyperparameter Tuning with GridSearchCV\n",
    "\n",
    "We use GridSearchCV to perform an exhaustive search over the specified hyperparameter grid. The model is evaluated using 5-fold cross-validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    }
   ],
   "source": [
    "# Use GridSearchCV for hyperparameter tuning\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1, verbose=1)\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 1000, 'subsample': 0.8}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 11. Get the Best Model from Grid Search\n",
    "\n",
    "After the grid search is complete, we retrieve the model with the best hyperparameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best model from grid search\n",
    "best_model = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 12. Predict on the Test Set\n",
    "\n",
    "We use the best model to make predictions on the test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the test set\n",
    "y_pred = best_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 13. Evaluate the Model\n",
    "\n",
    "We evaluate the model's performance using Mean Squared Error (MSE) and R² score. These metrics will give us an idea of how well the model is performing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 14.54\n",
      "R^2 Score: 0.94\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse:.2f}\")\n",
    "print(f\"R^2 Score: {r2:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 14. Plot Feature Importances\n",
    "\n",
    "We plot the feature importances to understand which features contributed the most to the model's predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot feature importances\n",
    "xgb.plot_importance(best_model)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 15. Save the Model\n",
    "\n",
    "Finally, we save the trained model to a file using `joblib`, so it can be loaded and used for predictions later without retraining.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['xgboost_model.pkl']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "# Save the model\n",
    "joblib.dump(best_model, \"xgboost_model.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Model\n",
    "\n",
    "This model has all the preprocessing and hyperparameter tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 14.54\n",
      "R^2 Score: 0.94\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import xgboost as xgb\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('Concrete_Data.csv')\n",
    "\n",
    "# Rename columns for easier access\n",
    "data = data.rename(columns={\n",
    "    'Cement (component 1)(kg in a m^3 mixture)': 'cement',\n",
    "    'Blast Furnace Slag (component 2)(kg in a m^3 mixture)': 'blast_furnace_slag',\n",
    "    'Fly Ash (component 3)(kg in a m^3 mixture)': 'fly_ash',\n",
    "    'Water  (component 4)(kg in a m^3 mixture)': 'water',\n",
    "    'Superplasticizer (component 5)(kg in a m^3 mixture)': 'superplasticizer',\n",
    "    'Coarse Aggregate  (component 6)(kg in a m^3 mixture)': 'coarse_aggregate',\n",
    "    'Fine Aggregate (component 7)(kg in a m^3 mixture)': 'fine_aggregate',\n",
    "    'Age (day)': 'age',\n",
    "    'Concrete compressive strength(MPa, megapascals) ': 'compressive_strength'\n",
    "})\n",
    "\n",
    "# Create additional features\n",
    "data['cement_coarse'] = data.cement / data.coarse_aggregate\n",
    "data['cement_fine'] = data.cement / data.fine_aggregate\n",
    "\n",
    "# Define features and target variable\n",
    "X = data.drop(['compressive_strength'], axis=1)\n",
    "y = data['compressive_strength']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create the XGBoost regressor model\n",
    "model = xgb.XGBRegressor(objective='reg:squarederror', \n",
    "                         eval_metric='rmse')\n",
    "\n",
    "model.set_params(n_estimators=1000, max_depth=3, learning_rate=0.1, subsample=0.8)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse:.2f}\")\n",
    "print(f\"R^2 Score: {r2:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "In this notebook, we've successfully built and evaluated an XGBoost model for predicting concrete compressive strength. The model was tuned using GridSearchCV, and the final model's performance was assessed using MSE and R² score. The feature importances were also visualized to understand the contribution of each feature to the model's predictions.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
